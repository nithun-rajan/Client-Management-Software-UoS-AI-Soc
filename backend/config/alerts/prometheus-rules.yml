# Prometheus Alert Rules
# These rules define alerts for monitoring the Estate Agent CRM backend

groups:
  - name: api_health
    interval: 30s
    rules:
      - alert: APIDown
        expr: up{job="estate-agent-crm"} == 0
        for: 1m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "API is down"
          description: "The Estate Agent CRM API has been down for more than 1 minute."

      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) /
          rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: high
          component: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      - alert: DatabaseConnectionFailure
        expr: |
          probe_success{job="estate-agent-crm-db"} == 0
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection failure"
          description: "Unable to connect to database for more than 2 minutes"

  - name: performance
    interval: 30s
    rules:
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1.0
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High API latency detected"
          description: "P95 latency is {{ $value }}s (threshold: 1s)"

      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket{endpoint=~"/api/.*"}[5m])
          ) > 2.0
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow database queries detected"
          description: "Database query P95 latency is {{ $value }}s (threshold: 2s)"

  - name: resource_usage
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 80%)"

      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 85%)"

      - alert: DiskSpaceRunningOut
        expr: |
          (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 15
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Low disk space"
          description: "Only {{ $value | humanize }}% disk space remaining (threshold: 15%)"

  - name: rate_limiting
    interval: 30s
    rules:
      - alert: HighRateLimitViolations
        expr: |
          rate(http_requests_total{status="429"}[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: security
        annotations:
          summary: "High rate of rate limit violations"
          description: "Rate limit violations: {{ $value | humanize }} per second"

      - alert: PotentialDDoS
        expr: |
          rate(http_requests_total{status="429"}[1m]) > 100
        for: 2m
        labels:
          severity: high
          component: security
        annotations:
          summary: "Potential DDoS attack"
          description: "Very high rate of rate limit violations: {{ $value | humanize }} per second"

  - name: security
    interval: 30s
    rules:
      - alert: HighAuthenticationFailureRate
        expr: |
          rate(http_requests_total{endpoint="/api/v1/login",status="401"}[5m]) > 5
        for: 5m
        labels:
          severity: high
          component: security
        annotations:
          summary: "High authentication failure rate"
          description: "Auth failures: {{ $value | humanize }} per second (possible brute force)"

      - alert: CSRFViolations
        expr: |
          rate(http_requests_total{status="403"}[5m]) > 2
        for: 5m
        labels:
          severity: medium
          component: security
        annotations:
          summary: "CSRF validation failures detected"
          description: "CSRF failures: {{ $value | humanize }} per second"

  - name: slo_violations
    interval: 1m
    rules:
      - alert: UptimeSLOViolation
        expr: |
          (1 - (sum(rate(http_requests_total{status=~"5.."}[30d])) /
           sum(rate(http_requests_total[30d])))) < 0.999
        for: 1h
        labels:
          severity: high
          component: slo
        annotations:
          summary: "Uptime SLO violation"
          description: "30-day uptime is {{ $value | humanizePercentage }} (SLO: 99.9%)"

      - alert: LatencySLOViolation
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket[30d])
          ) > 0.5
        for: 1h
        labels:
          severity: high
          component: slo
        annotations:
          summary: "Latency SLO violation"
          description: "30-day P95 latency is {{ $value }}s (SLO: 500ms)"

      - alert: ErrorRateSLOViolation
        expr: |
          (sum(rate(http_requests_total{status=~"5.."}[30d])) /
           sum(rate(http_requests_total[30d]))) > 0.001
        for: 1h
        labels:
          severity: high
          component: slo
        annotations:
          summary: "Error rate SLO violation"
          description: "30-day error rate is {{ $value | humanizePercentage }} (SLO: 0.1%)"

      # Error Budget Burn Rate Alerts
      # Multi-window, multi-burn-rate alerting for faster incident detection

      - alert: ErrorBudgetBurnRateCritical
        expr: |
          # Burn rate > 14.4x over 1 hour AND > 14.4x over 5 minutes
          # This will exhaust 30-day budget in ~2 hours
          (
            (1 - (sum(rate(http_requests_total{status!~"5.."}[1h])) / sum(rate(http_requests_total[1h])))) / (1 - 0.999)
            > 14.4
          )
          and
          (
            (1 - (sum(rate(http_requests_total{status!~"5.."}[5m])) / sum(rate(http_requests_total[5m])))) / (1 - 0.999)
            > 14.4
          )
        for: 2m
        labels:
          severity: critical
          component: slo
          burn_rate: "fast"
        annotations:
          summary: "Critical error budget burn rate detected"
          description: "Error budget burning at 14.4x rate. Budget will be exhausted in ~2 hours at current rate."
          action: "Immediate investigation required. Consider rolling back recent changes."

      - alert: ErrorBudgetBurnRateHigh
        expr: |
          # Burn rate > 6x over 6 hours AND > 6x over 30 minutes
          # This will exhaust 30-day budget in ~5 days
          (
            (1 - (sum(rate(http_requests_total{status!~"5.."}[6h])) / sum(rate(http_requests_total[6h])))) / (1 - 0.999)
            > 6
          )
          and
          (
            (1 - (sum(rate(http_requests_total{status!~"5.."}[30m])) / sum(rate(http_requests_total[30m])))) / (1 - 0.999)
            > 6
          )
        for: 15m
        labels:
          severity: high
          component: slo
          burn_rate: "medium"
        annotations:
          summary: "High error budget burn rate detected"
          description: "Error budget burning at 6x rate. Budget will be exhausted in ~5 days at current rate."
          action: "Investigation recommended. Monitor closely and prepare mitigation."

      - alert: ErrorBudgetBurnRateElevated
        expr: |
          # Burn rate > 3x over 24 hours AND > 3x over 2 hours
          # This will exhaust 30-day budget in ~10 days
          (
            (1 - (sum(rate(http_requests_total{status!~"5.."}[24h])) / sum(rate(http_requests_total[24h])))) / (1 - 0.999)
            > 3
          )
          and
          (
            (1 - (sum(rate(http_requests_total{status!~"5.."}[2h])) / sum(rate(http_requests_total[2h])))) / (1 - 0.999)
            > 3
          )
        for: 1h
        labels:
          severity: warning
          component: slo
          burn_rate: "slow"
        annotations:
          summary: "Elevated error budget burn rate"
          description: "Error budget burning at 3x rate. Budget will be exhausted in ~10 days at current rate."
          action: "Review recent changes and trends. Plan corrective actions."

      - alert: ErrorBudgetExhausted
        expr: |
          # Error budget completely consumed
          (1 - (sum(rate(http_requests_total{status!~"5.."}[30d])) / sum(rate(http_requests_total[30d])))) >= (1 - 0.999)
        for: 5m
        labels:
          severity: critical
          component: slo
        annotations:
          summary: "Error budget exhausted"
          description: "30-day error budget has been completely consumed. No margin for errors remaining."
          action: "Halt non-critical deployments. Focus on reliability improvements."

  - name: business_metrics
    interval: 1m
    rules:
      - alert: NoRecentRequests
        expr: |
          rate(http_requests_total[5m]) == 0
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "No API requests received"
          description: "No requests in the last 10 minutes (possible issue or very low traffic)"

      - alert: SuddenTrafficSpike
        expr: |
          rate(http_requests_total[5m]) >
          (avg_over_time(rate(http_requests_total[5m])[1h:5m]) * 3)
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Sudden traffic spike detected"
          description: "Request rate is 3x higher than average: {{ $value | humanize }} req/s"

      - alert: DatabaseConnectionPoolExhausted
        expr: |
          # Placeholder - adjust based on actual connection pool metrics
          # Example: db_connection_pool_size - db_connection_pool_available < 2
          up{job="estate-agent-crm"} == 1
        for: 5m
        labels:
          severity: high
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Less than 2 database connections available"
